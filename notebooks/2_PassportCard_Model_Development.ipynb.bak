{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PassportCard Model DevelopmentThis notebook builds on the data exploration from notebook 1, focusing on feature engineering and predictive model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Processed Data\n",
    "\n",
    "We'll load the cleaned data produced in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claims data shape: (500, 9)\n",
      "Members data shape: (50, 25)\n"
     ]
    }
   ],
   "source": [
    "# Load claims and member data\n",
    "claims_data = pd.read_csv('../claims_data_clean.csv')\n",
    "members_data = pd.read_csv('../members_data_clean.csv')\n",
    "\n",
    "# Convert date columns to datetime\n",
    "claims_data['ServiceDate'] = pd.to_datetime(claims_data['ServiceDate'])\n",
    "claims_data['PayDate'] = pd.to_datetime(claims_data['PayDate'])\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Claims data shape: {claims_data.shape}\")\n",
    "print(f\"Members data shape: {members_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Let's prepare features for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling data shape: (50, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Member_ID</th>\n",
       "      <th>ClaimCount</th>\n",
       "      <th>MeanClaimAmount</th>\n",
       "      <th>TotalClaimAmount</th>\n",
       "      <th>ClaimAmountStd</th>\n",
       "      <th>TenureDays</th>\n",
       "      <th>ClaimFrequency</th>\n",
       "      <th>PolicyID</th>\n",
       "      <th>PolicyStartDate</th>\n",
       "      <th>PolicyEndDate</th>\n",
       "      <th>...</th>\n",
       "      <th>Questionnaire_immune</th>\n",
       "      <th>Questionnaire_tumor</th>\n",
       "      <th>Questionnaire_relatives</th>\n",
       "      <th>Questionnaire_alcoholism</th>\n",
       "      <th>Questionnaire_drink</th>\n",
       "      <th>uw_pct</th>\n",
       "      <th>average_us</th>\n",
       "      <th>average_ov</th>\n",
       "      <th>average_ob</th>\n",
       "      <th>FutureClaimAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1883.916917</td>\n",
       "      <td>22607.003003</td>\n",
       "      <td>2532.777690</td>\n",
       "      <td>902</td>\n",
       "      <td>0.399113</td>\n",
       "      <td>100</td>\n",
       "      <td>2018-04-13</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.347262</td>\n",
       "      <td>0.307539</td>\n",
       "      <td>0.233027</td>\n",
       "      <td>0.299734</td>\n",
       "      <td>1359.561381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1021.310987</td>\n",
       "      <td>14298.353824</td>\n",
       "      <td>2045.486861</td>\n",
       "      <td>628</td>\n",
       "      <td>0.668790</td>\n",
       "      <td>101</td>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>2021-08-29</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.598563</td>\n",
       "      <td>0.403754</td>\n",
       "      <td>0.690427</td>\n",
       "      <td>0.491540</td>\n",
       "      <td>732.011805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1302.818599</td>\n",
       "      <td>15633.823192</td>\n",
       "      <td>2251.387980</td>\n",
       "      <td>884</td>\n",
       "      <td>0.407240</td>\n",
       "      <td>101</td>\n",
       "      <td>2018-09-28</td>\n",
       "      <td>2021-12-11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.663291</td>\n",
       "      <td>0.572236</td>\n",
       "      <td>0.166239</td>\n",
       "      <td>0.276968</td>\n",
       "      <td>961.992194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1343.865022</td>\n",
       "      <td>10750.920176</td>\n",
       "      <td>1728.392240</td>\n",
       "      <td>576</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>102</td>\n",
       "      <td>2018-04-17</td>\n",
       "      <td>2022-07-18</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.179933</td>\n",
       "      <td>0.314451</td>\n",
       "      <td>0.582522</td>\n",
       "      <td>0.890134</td>\n",
       "      <td>1034.594596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1689.638791</td>\n",
       "      <td>15206.749117</td>\n",
       "      <td>2785.661929</td>\n",
       "      <td>1016</td>\n",
       "      <td>0.265748</td>\n",
       "      <td>102</td>\n",
       "      <td>2018-03-13</td>\n",
       "      <td>2021-12-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.286585</td>\n",
       "      <td>0.599319</td>\n",
       "      <td>0.296279</td>\n",
       "      <td>0.855247</td>\n",
       "      <td>1184.212320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Member_ID  ClaimCount  MeanClaimAmount  TotalClaimAmount  ClaimAmountStd  \\\n",
       "0          1          12      1883.916917      22607.003003     2532.777690   \n",
       "1          2          14      1021.310987      14298.353824     2045.486861   \n",
       "2          3          12      1302.818599      15633.823192     2251.387980   \n",
       "3          4           8      1343.865022      10750.920176     1728.392240   \n",
       "4          5           9      1689.638791      15206.749117     2785.661929   \n",
       "\n",
       "   TenureDays  ClaimFrequency  PolicyID PolicyStartDate PolicyEndDate  ...  \\\n",
       "0         902        0.399113       100      2018-04-13    2021-01-13  ...   \n",
       "1         628        0.668790       101      2018-12-15    2021-08-29  ...   \n",
       "2         884        0.407240       101      2018-09-28    2021-12-11  ...   \n",
       "3         576        0.416667       102      2018-04-17    2022-07-18  ...   \n",
       "4        1016        0.265748       102      2018-03-13    2021-12-05  ...   \n",
       "\n",
       "  Questionnaire_immune Questionnaire_tumor Questionnaire_relatives  \\\n",
       "0                    0                   0                       0   \n",
       "1                    0                   0                       0   \n",
       "2                    0                   0                       0   \n",
       "3                    0                   1                       0   \n",
       "4                    0                   0                       0   \n",
       "\n",
       "   Questionnaire_alcoholism  Questionnaire_drink    uw_pct  average_us  \\\n",
       "0                         1                    0  1.347262    0.307539   \n",
       "1                         0                    0  0.598563    0.403754   \n",
       "2                         0                    1  1.663291    0.572236   \n",
       "3                         0                    0  1.179933    0.314451   \n",
       "4                         0                    0  1.286585    0.599319   \n",
       "\n",
       "   average_ov  average_ob  FutureClaimAmount  \n",
       "0    0.233027    0.299734        1359.561381  \n",
       "1    0.690427    0.491540         732.011805  \n",
       "2    0.166239    0.276968         961.992194  \n",
       "3    0.582522    0.890134        1034.594596  \n",
       "4    0.296279    0.855247        1184.212320  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_features(claims_df, members_df):\n",
    "    \"\"\"Prepare features for modeling\"\"\"\n",
    "    # Work with copies\n",
    "    claims = claims_df.copy()\n",
    "    members = members_df.copy()\n",
    "    \n",
    "    # Basic temporal features\n",
    "    claims['Year'] = claims['ServiceDate'].dt.year\n",
    "    claims['Month'] = claims['ServiceDate'].dt.month\n",
    "    claims['DayOfWeek'] = claims['ServiceDate'].dt.dayofweek\n",
    "    \n",
    "    # Cyclical encoding for month and day of week\n",
    "    claims['Month_sin'] = np.sin(2 * np.pi * claims['Month'] / 12)\n",
    "    claims['Month_cos'] = np.cos(2 * np.pi * claims['Month'] / 12)\n",
    "    claims['DayOfWeek_sin'] = np.sin(2 * np.pi * claims['DayOfWeek'] / 7)\n",
    "    claims['DayOfWeek_cos'] = np.cos(2 * np.pi * claims['DayOfWeek'] / 7)\n",
    "    \n",
    "    # Aggregate to member level\n",
    "    member_features = claims.groupby('Member_ID').agg({\n",
    "        'TotPaymentUSD': ['count', 'mean', 'sum', 'std'],\n",
    "        'ServiceDate': ['min', 'max']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten multi-level column names\n",
    "    member_features.columns = ['_'.join(col).strip('_') for col in member_features.columns.values]\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    member_features = member_features.rename(columns={\n",
    "        'Member_ID': 'Member_ID',\n",
    "        'TotPaymentUSD_count': 'ClaimCount',\n",
    "        'TotPaymentUSD_mean': 'MeanClaimAmount',\n",
    "        'TotPaymentUSD_sum': 'TotalClaimAmount',\n",
    "        'TotPaymentUSD_std': 'ClaimAmountStd',\n",
    "        'ServiceDate_min': 'FirstClaimDate',\n",
    "        'ServiceDate_max': 'LastClaimDate'\n",
    "    })\n",
    "    \n",
    "    # Calculate member tenure (days between first and last claim)\n",
    "    member_features['TenureDays'] = (member_features['LastClaimDate'] - member_features['FirstClaimDate']).dt.days\n",
    "    \n",
    "    # Calculate claim frequency (claims per month)\n",
    "    member_features['ClaimFrequency'] = np.where(\n",
    "        member_features['TenureDays'] > 0,\n",
    "        member_features['ClaimCount'] / (member_features['TenureDays'] / 30),\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Merge with member data\n",
    "    data = pd.merge(member_features, members, on='Member_ID', how='left')\n",
    "    \n",
    "    # Create target variable: future claims (this would be calculated from additional data in a real scenario)\n",
    "    # For demonstration, we'll use a simple function of existing features plus random noise\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['FutureClaimAmount'] = (\n",
    "        0.7 * data['MeanClaimAmount'] + \n",
    "        0.3 * data['ClaimFrequency'] * 100 +\n",
    "        0.2 * data['BMI'] +\n",
    "        np.random.normal(0, 50, size=len(data))\n",
    "    )\n",
    "    \n",
    "    # Ensure non-negative values\n",
    "    data['FutureClaimAmount'] = data['FutureClaimAmount'].clip(lower=0)\n",
    "    \n",
    "    # Drop date columns for modeling\n",
    "    data = data.drop(columns=['FirstClaimDate', 'LastClaimDate'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Prepare features for modeling\n",
    "modeling_data = prepare_features(claims_data, members_data)\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Modeling data shape: {modeling_data.shape}\")\n",
    "modeling_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (40, 23)\n",
      "Testing set shape: (10, 23)\n",
      "Number of features: 29\n",
      "Log-transformed target: True\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_modeling(df, target_col='FutureClaimAmount', test_size=0.2, log_transform=True):\n",
    "    \"\"\"Prepare data for modeling by splitting and transforming\"\"\"\n",
    "    # Work with a copy\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Apply log transform to target if specified\n",
    "    if log_transform:\n",
    "        data['Log_' + target_col] = np.log1p(data[target_col])\n",
    "        y_col = 'Log_' + target_col\n",
    "    else:\n",
    "        y_col = target_col\n",
    "    \n",
    "    # Select features and target\n",
    "    feature_cols = [\n",
    "        col for col in data.columns \n",
    "        if col not in [target_col, 'Log_' + target_col, 'Member_ID', 'PolicyID']\n",
    "    ]\n",
    "    \n",
    "    X = data[feature_cols]\n",
    "    # Filter out non-numeric columns to avoid conversion errors\n",
    "    X = X.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    y = data[y_col]\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale numerical features\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    scaler = RobustScaler()  # Robust to outliers\n",
    "    \n",
    "    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, feature_cols, log_transform\n",
    "\n",
    "# Prepare data for modeling\n",
    "X_train, X_test, y_train, y_test, feature_cols, log_transform = prepare_for_modeling(\n",
    "    modeling_data, target_col='FutureClaimAmount', log_transform=True\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Log-transformed target: {log_transform}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "We'll evaluate several regression models to select the best performing one for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Regression...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     result, trained_model, y_pred = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_transform\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     results_list.append(result)\n\u001b[32m     89\u001b[39m     trained_models[name] = trained_model\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(name, model, X_train, y_train, X_test, y_test, log_transform)\u001b[39m\n\u001b[32m      4\u001b[39m start_time = time.time()\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Calculate training time\u001b[39;00m\n\u001b[32m     10\u001b[39m train_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\DS_assignment_passportcard\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:648\u001b[39m, in \u001b[36mLinearRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    644\u001b[39m n_jobs_ = \u001b[38;5;28mself\u001b[39m.n_jobs\n\u001b[32m    646\u001b[39m accept_sparse = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.positive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcoo\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m X, y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m sample_weight = _check_sample_weight(\n\u001b[32m    653\u001b[39m     sample_weight, X, dtype=X.dtype, only_non_negative=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    654\u001b[39m )\n\u001b[32m    656\u001b[39m X, y, X_offset, y_offset, X_scale = _preprocess_data(\n\u001b[32m    657\u001b[39m     X,\n\u001b[32m    658\u001b[39m     y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    661\u001b[39m     sample_weight=sample_weight,\n\u001b[32m    662\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\DS_assignment_passportcard\\venv\\Lib\\site-packages\\sklearn\\base.py:584\u001b[39m, in \u001b[36mBaseEstimator._validate_data\u001b[39m\u001b[34m(self, X, y, reset, validate_separately, **check_params)\u001b[39m\n\u001b[32m    582\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m    583\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    585\u001b[39m     out = X, y\n\u001b[32m    587\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\DS_assignment_passportcard\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1101\u001b[39m         estimator_name = _check_estimator_name(estimator)\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1103\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1106\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1122\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1124\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\DS_assignment_passportcard\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    915\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    916\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    917\u001b[39m             % (array.ndim, estimator_name)\n\u001b[32m    918\u001b[39m         )\n\u001b[32m    920\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples > \u001b[32m0\u001b[39m:\n\u001b[32m    929\u001b[39m     n_samples = _num_samples(array)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\DS_assignment_passportcard\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:161\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    145\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    146\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    147\u001b[39m     msg_err += (\n\u001b[32m    148\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    149\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    160\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test, log_transform=True):\n",
    "    \"\"\"Train and evaluate a model\"\"\"\n",
    "    # Time training process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate training time\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Transform predictions back to original scale if log-transformed\n",
    "    if log_transform:\n",
    "        y_pred_train_orig = np.expm1(y_pred_train)\n",
    "        y_pred_test_orig = np.expm1(y_pred_test)\n",
    "        y_train_orig = np.expm1(y_train)\n",
    "        y_test_orig = np.expm1(y_test)\n",
    "    else:\n",
    "        y_pred_train_orig = y_pred_train\n",
    "        y_pred_test_orig = y_pred_test\n",
    "        y_train_orig = y_train\n",
    "        y_test_orig = y_test\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train_orig))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test_orig, y_pred_test_orig))\n",
    "    \n",
    "    mae_train = mean_absolute_error(y_train_orig, y_pred_train_orig)\n",
    "    mae_test = mean_absolute_error(y_test_orig, y_pred_test_orig)\n",
    "    \n",
    "    r2_train = r2_score(y_train_orig, y_pred_train_orig)\n",
    "    r2_test = r2_score(y_test_orig, y_pred_test_orig)\n",
    "    \n",
    "    # Calculate MAPE (Mean Absolute Percentage Error) for values > 10\n",
    "    # to avoid division by very small values\n",
    "    train_idx = y_train_orig > 10\n",
    "    test_idx = y_test_orig > 10\n",
    "    \n",
    "    if any(train_idx):\n",
    "        mape_train = np.mean(np.abs((y_train_orig[train_idx] - y_pred_train_orig[train_idx]) / y_train_orig[train_idx])) * 100\n",
    "    else:\n",
    "        mape_train = np.nan\n",
    "        \n",
    "    if any(test_idx):\n",
    "        mape_test = np.mean(np.abs((y_test_orig[test_idx] - y_pred_test_orig[test_idx]) / y_test_orig[test_idx])) * 100\n",
    "    else:\n",
    "        mape_test = np.nan\n",
    "    \n",
    "    # Organize results\n",
    "    results = {\n",
    "        'Model': name,\n",
    "        'RMSE_Train': rmse_train,\n",
    "        'RMSE_Test': rmse_test,\n",
    "        'MAE_Train': mae_train,\n",
    "        'MAE_Test': mae_test,\n",
    "        'R2_Train': r2_train,\n",
    "        'R2_Test': r2_test,\n",
    "        'MAPE_Train': mape_train,\n",
    "        'MAPE_Test': mape_test,\n",
    "        'Training_Time': train_time\n",
    "    }\n",
    "    \n",
    "    return results, model, y_pred_test_orig\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "results_list = []\n",
    "predictions = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    result, trained_model, y_pred = evaluate_model(\n",
    "        name, model, X_train, y_train, X_test, y_test, log_transform\n",
    "    )\n",
    "    results_list.append(result)\n",
    "    trained_models[name] = trained_model\n",
    "    predictions[name] = y_pred\n",
    "    print(f\"  RMSE: {result['RMSE_Test']:.2f}, MAE: {result['MAE_Test']:.2f}, R²: {result['R2_Test']:.3f}\")\n",
    "\n",
    "# Collect results in a DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
