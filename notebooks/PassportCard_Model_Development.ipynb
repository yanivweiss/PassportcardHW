{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PassportCard Insurance Claims Prediction: Model Development\n",
    "\n",
    "This notebook focuses on developing and evaluating machine learning models for the PassportCard insurance claims prediction project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Processed Data\n",
    "\n",
    "We'll load the cleaned data produced in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load claims and member data\n",
    "claims_data = pd.read_csv('claims_data_clean.csv')\n",
    "members_data = pd.read_csv('members_data_clean.csv')\n",
    "\n",
    "# Convert date columns to datetime\n",
    "claims_data['ServiceDate'] = pd.to_datetime(claims_data['ServiceDate'])\n",
    "claims_data['PayDate'] = pd.to_datetime(claims_data['PayDate'])\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Claims data shape: {claims_data.shape}\")\n",
    "print(f\"Members data shape: {members_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Let's prepare features for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(claims_df, members_df):\n",
    "    \"\"\"Prepare features for modeling\"\"\"\n",
    "    # Work with copies\n",
    "    claims = claims_df.copy()\n",
    "    members = members_df.copy()\n",
    "    \n",
    "    # Basic temporal features\n",
    "    claims['Year'] = claims['ServiceDate'].dt.year\n",
    "    claims['Month'] = claims['ServiceDate'].dt.month\n",
    "    claims['DayOfWeek'] = claims['ServiceDate'].dt.dayofweek\n",
    "    \n",
    "    # Cyclical encoding for month and day of week\n",
    "    claims['Month_sin'] = np.sin(2 * np.pi * claims['Month'] / 12)\n",
    "    claims['Month_cos'] = np.cos(2 * np.pi * claims['Month'] / 12)\n",
    "    claims['DayOfWeek_sin'] = np.sin(2 * np.pi * claims['DayOfWeek'] / 7)\n",
    "    claims['DayOfWeek_cos'] = np.cos(2 * np.pi * claims['DayOfWeek'] / 7)\n",
    "    \n",
    "    # Aggregate to member level\n",
    "    member_features = claims.groupby('Member_ID').agg({\n",
    "        'TotPaymentUSD': ['count', 'mean', 'sum', 'std'],\n",
    "        'ServiceDate': ['min', 'max']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten multi-level column names\n",
    "    member_features.columns = ['_'.join(col).strip('_') for col in member_features.columns.values]\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    member_features = member_features.rename(columns={\n",
    "        'Member_ID': 'Member_ID',\n",
    "        'TotPaymentUSD_count': 'ClaimCount',\n",
    "        'TotPaymentUSD_mean': 'MeanClaimAmount',\n",
    "        'TotPaymentUSD_sum': 'TotalClaimAmount',\n",
    "        'TotPaymentUSD_std': 'ClaimAmountStd',\n",
    "        'ServiceDate_min': 'FirstClaimDate',\n",
    "        'ServiceDate_max': 'LastClaimDate'\n",
    "    })\n",
    "    \n",
    "    # Calculate member tenure (days between first and last claim)\n",
    "    member_features['TenureDays'] = (member_features['LastClaimDate'] - member_features['FirstClaimDate']).dt.days\n",
    "    \n",
    "    # Calculate claim frequency (claims per month)\n",
    "    member_features['ClaimFrequency'] = np.where(\n",
    "        member_features['TenureDays'] > 0,\n",
    "        member_features['ClaimCount'] / (member_features['TenureDays'] / 30),\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Merge with member data\n",
    "    data = pd.merge(member_features, members, on='Member_ID', how='left')\n",
    "    \n",
    "    # Create target variable: future claims (this would be calculated from additional data in a real scenario)\n",
    "    # For demonstration, we'll use a simple function of existing features plus random noise\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['FutureClaimAmount'] = (\n",
    "        0.7 * data['MeanClaimAmount'] + \n",
    "        0.3 * data['ClaimFrequency'] * 100 +\n",
    "        0.2 * data['BMI'] +\n",
    "        np.random.normal(0, 50, size=len(data))\n",
    "    )\n",
    "    \n",
    "    # Ensure non-negative values\n",
    "    data['FutureClaimAmount'] = data['FutureClaimAmount'].clip(lower=0)\n",
    "    \n",
    "    # Drop date columns for modeling\n",
    "    data = data.drop(columns=['FirstClaimDate', 'LastClaimDate'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Prepare features for modeling\n",
    "modeling_data = prepare_features(claims_data, members_data)\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Modeling data shape: {modeling_data.shape}\")\n",
    "modeling_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_modeling(df, target_col='FutureClaimAmount', test_size=0.2, log_transform=True):\n",
    "    \"\"\"Prepare data for modeling by splitting and transforming\"\"\"\n",
    "    # Work with a copy\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Apply log transform to target if specified\n",
    "    if log_transform:\n",
    "        data['Log_' + target_col] = np.log1p(data[target_col])\n",
    "        y_col = 'Log_' + target_col\n",
    "    else:\n",
    "        y_col = target_col\n",
    "    \n",
    "    # Select features and target\n",
    "    feature_cols = [\n",
    "        col for col in data.columns \n",
    "        if col not in [target_col, 'Log_' + target_col, 'Member_ID', 'PolicyID']\n",
    "    ]\n",
    "    \n",
    "    X = data[feature_cols]\n",
    "    y = data[y_col]\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale numerical features\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    scaler = RobustScaler()  # Robust to outliers\n",
    "    \n",
    "    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, feature_cols, log_transform\n",
    "\n",
    "# Prepare data for modeling\n",
    "X_train, X_test, y_train, y_test, feature_cols, log_transform = prepare_for_modeling(\n",
    "    modeling_data, target_col='FutureClaimAmount', log_transform=True\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Log-transformed target: {log_transform}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "We'll evaluate several regression models to select the best performing one for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test, log_transform=True):\n",
    "    \"\"\"Train and evaluate a model\"\"\"\n",
    "    # Time training process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate training time\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Transform predictions back to original scale if log-transformed\n",
    "    if log_transform:\n",
    "        y_pred_train_orig = np.expm1(y_pred_train)\n",
    "        y_pred_test_orig = np.expm1(y_pred_test)\n",
    "        y_train_orig = np.expm1(y_train)\n",
    "        y_test_orig = np.expm1(y_test)\n",
    "    else:\n",
    "        y_pred_train_orig = y_pred_train\n",
    "        y_pred_test_orig = y_pred_test\n",
    "        y_train_orig = y_train\n",
    "        y_test_orig = y_test\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train_orig))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test_orig, y_pred_test_orig))\n",
    "    \n",
    "    mae_train = mean_absolute_error(y_train_orig, y_pred_train_orig)\n",
    "    mae_test = mean_absolute_error(y_test_orig, y_pred_test_orig)\n",
    "    \n",
    "    r2_train = r2_score(y_train_orig, y_pred_train_orig)\n",
    "    r2_test = r2_score(y_test_orig, y_pred_test_orig)\n",
    "    \n",
    "    # Calculate MAPE (Mean Absolute Percentage Error) for values > 10\n",
    "    # to avoid division by very small values\n",
    "    train_idx = y_train_orig > 10\n",
    "    test_idx = y_test_orig > 10\n",
    "    \n",
    "    if any(train_idx):\n",
    "        mape_train = np.mean(np.abs((y_train_orig[train_idx] - y_pred_train_orig[train_idx]) / y_train_orig[train_idx])) * 100\n",
    "    else:\n",
    "        mape_train = np.nan\n",
    "        \n",
    "    if any(test_idx):\n",
    "        mape_test = np.mean(np.abs((y_test_orig[test_idx] - y_pred_test_orig[test_idx]) / y_test_orig[test_idx])) * 100\n",
    "    else:\n",
    "        mape_test = np.nan\n",
    "    \n",
    "    # Organize results\n",
    "    results = {\n",
    "        'Model': name,\n",
    "        'RMSE_Train': rmse_train,\n",
    "        'RMSE_Test': rmse_test,\n",
    "        'MAE_Train': mae_train,\n",
    "        'MAE_Test': mae_test,\n",
    "        'R2_Train': r2_train,\n",
    "        'R2_Test': r2_test,\n",
    "        'MAPE_Train': mape_train,\n",
    "        'MAPE_Test': mape_test,\n",
    "        'Training_Time': train_time\n",
    "    }\n",
    "    \n",
    "    return results, model, y_pred_test_orig\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "results_list = []\n",
    "predictions = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    result, trained_model, y_pred = evaluate_model(\n",
    "        name, model, X_train, y_train, X_test, y_test, log_transform\n",
    "    )\n",
    "    results_list.append(result)\n",
    "    trained_models[name] = trained_model\n",
    "    predictions[name] = y_pred\n",
    "    print(f\"  RMSE: {result['RMSE_Test']:.2f}, MAE: {result['MAE_Test']:.2f}, RÂ²: {result['R2_Test']:.3f}\")\n",
    "\n",
    "# Collect results in a DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
